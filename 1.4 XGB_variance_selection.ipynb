{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616ff0ac-4d66-419d-b7d2-c54c20e931d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XG Boost with variance based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dfb751e-73c4-404a-8201-8a2a261bbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "# Plot the figures inline, necessary only for Jupyter notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import os # miscelleaneous operating system interface\n",
    "import numpy as np  # import numpy\n",
    "import pandas as pd # import pandas\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # import seaborn for data visualization\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as RMSE # calculate RMSE\n",
    "from sklearn.model_selection import train_test_split # splitting data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701c0ce7-f724-4d44-b92f-3912a5edd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data set for drugs\n",
    "\n",
    "df_drug=pd.read_csv('GDSC2_label_14drugs.csv') # load the data set for drugs (limited to 14 drugs)\n",
    "df_drug.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "# print(df_drug.shape) # shape for drug data\n",
    "\n",
    "# Result: 805 tumor cells (cell lines) and 14 drugs\n",
    "# df_drug.head(5) # print the first 5 instances to have a look\n",
    "\n",
    "# We only focus on the 3 drugs with largest variances in their efficacies among different drugs\n",
    "drug_sort=df_drug.std().sort_values(ascending = False).iloc[0:3]\n",
    "\n",
    "# import the data set for tumor cells (cell lines) and genes\n",
    "\n",
    "df_tumor=pd.read_csv('GDSC2_expression14.csv') # load the data set for tumors and cell lines\n",
    "df_tumor.set_index('Unnamed: 0', inplace=True)\n",
    "gene=list(df_tumor.columns)\n",
    "\n",
    "# print('Number of genes:', len(gene))\n",
    "# print('First gene: ',gene[0])\n",
    "\n",
    "# print('Shape of data frame', df_tumor.shape) # shape for tumor data\n",
    "# df_tumor.iloc[0:5, 0:10] # print the first 5 instances to have a look, only print the first 10 columns\n",
    "\n",
    "# Result: 805 tumor cells (instances) and 17419 genes (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42dea26f-df9c-400a-bb2a-37aaff266633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 drugs with the largest variances among all 805 tumors\n",
    "# See EDA and overfitting notebook\n",
    "\n",
    "drug1=drug_sort.index[0] # 'Docetaxel'\n",
    "drug2=drug_sort.index[1] # 'Trametinib'\n",
    "drug3=drug_sort.index[2] # 'Entinostat'\n",
    "\n",
    "# Merge the two data set together WITHOUT any selection of features\n",
    "# We do not need to save too many decimal places, keep 2 decimal places is fine\n",
    "# The last column becomes the drug efficacy\n",
    "\n",
    "df_1=pd.concat([df_tumor, df_drug[drug1].round(2)], axis=1) # axis=1 because we join the columns, not rows\n",
    "df_2=pd.concat([df_tumor, df_drug[drug2].round(2)], axis=1) \n",
    "df_3=pd.concat([df_tumor, df_drug[drug3].round(2)], axis=1) \n",
    "\n",
    "# print(df_1.shape)\n",
    "# df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c399b2-9e43-479d-8909-0754da2a56af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RPS4Y1', 'HLA-DRA', 'ITM2A', 'MIR205HG', 'TACSTD2', 'SPP1', 'TSPAN8',\n",
       "       'LAPTM5', 'TFF1', 'GMFG', 'COL1A2', 'KRT6A', 'LUM', 'S100A9', 'BEX1',\n",
       "       'SRGN', 'CD53', 'IGJ', 'POU2AF1', 'S100A14', 'TFF3', 'S100P', 'GTSF1',\n",
       "       'AKR1B10P1', 'COL3A1', 'C8orf4', 'UCA1', 'INSM1', 'GPX2', 'LCN2',\n",
       "       'CEACAM5', 'MMP7', 'PCP4', 'CD52', 'LYZ', 'AKR1C2', 'FGFBP1', 'GRP',\n",
       "       'PHGR1', 'MMP1', 'SCG2', 'RAB25', 'CXCR4', 'COL6A3', 'KRT5', 'POSTN',\n",
       "       'BCL2A1', 'INHBB', 'CCL2', 'ANXA10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate variance for each gene across the 805 samples\n",
    "# Select the 100 genes with highest variance (after normalization)\n",
    "\n",
    "from sklearn.preprocessing import normalize # normalize the columns for the genes\n",
    "\n",
    "n=50 # number of genes to keep\n",
    "\n",
    "df_tumor_norm=pd.DataFrame(normalize(df_tumor, axis=0)) # result after normalization is a numpy array, we need data frame\n",
    "df_tumor_norm.columns=gene # assign the gene name as column names\n",
    "\n",
    "# calculate the variance for each gene type across 805 samples and sort the results\n",
    "df_tumor_var=pd.DataFrame(df_tumor_norm.var())\n",
    "df_tumor_var.columns=['normed var']\n",
    "\n",
    "# Comments: There are many genes having small variances across different types of tumor cells. \n",
    "# Again, doesn't mean that they have no importance in drug efficacy!\n",
    "\n",
    "# Picking the 50 genes with the largest variances across all tumor types\n",
    "\n",
    "df_var=df_tumor_var.sort_values('normed var',ascending = False).iloc[0:n,:]\n",
    "df_var.index # This list stores the names of that 50 genes\n",
    "# df_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02f5fe-9f4f-4c15-b062-c5fcb6e85e4f",
   "metadata": {},
   "source": [
    "# XG Boost with hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee94e84-67a2-4d22-897c-2756884f34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /Users/kenkwma/Library/Python/3.10/lib/python/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/kenkwma/Library/Python/3.10/lib/python/site-packages (from xgboost) (1.24.1)\n",
      "Requirement already satisfied: scipy in /Users/kenkwma/Library/Python/3.10/lib/python/site-packages (from xgboost) (1.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install XG Boost packages to local kernel\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c71fe-ac5c-46ad-b955-6c3f0e648677",
   "metadata": {},
   "source": [
    "4.3.1 For Docetaxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8352a40-0e20-4200-9249-82b215ebed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n",
      "Best parameters found: {'colsample_bytree': 0.8, 'gamma': 0.5, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}\n",
      "Training RMSE for Docetaxel: 0.177\n",
      "Test RMSE for Docetaxel: 0.167\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train_1, df_test_1 = train_test_split(df_1, shuffle=True, random_state=42, test_size=.2) # For Docetaxel\n",
    "\n",
    "params = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 5, 10], # minimum number of instances needed to be in each node\n",
    "    'gamma': [0.5, 1, 1.5], # minimum loss reduction required to make a further partition\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'learning_rate': [0.05, 0.1, 0.25, 0.5]\n",
    "}\n",
    "\n",
    "# cv : number of k-fold cross validation\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective ='reg:squarederror'),\n",
    "                           param_grid=params,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=3,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(df_train_1[df_var.index], df_train_1.iloc[:,-1:])\n",
    "print(f'Best parameters found: {grid_search.best_params_}')\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', **best_params)\n",
    "model.fit(df_train_1[df_var.index], df_train_1.iloc[:,-1:])\n",
    "\n",
    "# Prediction on training data\n",
    "y_train_pred = model.predict(df_train_1[df_var.index])\n",
    "train_rmse = np.round(RMSE(df_train_1.iloc[:,-1:], y_train_pred),3)\n",
    "\n",
    "# Prediction on test data\n",
    "y_test_pred = model.predict(df_test_1[df_var.index])\n",
    "test_rmse = np.round(RMSE(df_test_1.iloc[:,-1:], y_test_pred),3)\n",
    "\n",
    "print(f'Training RMSE for {drug1}: {train_rmse}')\n",
    "print(f'Test RMSE for {drug1}: {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fb6d5-7f0a-42e6-b991-5b6e6115389f",
   "metadata": {},
   "source": [
    "4.3.2 For Trametinib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696aef53-309a-4eec-8e66-b174bc29a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n",
      "Best parameters found: {'colsample_bytree': 1.0, 'gamma': 0.5, 'learning_rate': 0.75, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}\n",
      "Training RMSE for Trametinib: 0.14\n",
      "Test RMSE Trametinib: 0.192\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train_2, df_test_2 = train_test_split(df_2, shuffle=True, random_state=42, test_size=.2) # For Trametinib\n",
    "\n",
    "params = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 5, 10], # minimum number of instances needed to be in each node\n",
    "    'gamma': [0.5, 1, 1.5], # minimum loss reduction required to make a further partition\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'learning_rate': [0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# cv : number of k-fold cross validation\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective ='reg:squarederror'),\n",
    "                           param_grid=params,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=3,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(df_train_2[df_var.index], df_train_2.iloc[:,-1:])\n",
    "print(f'Best parameters found: {grid_search.best_params_}')\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', **best_params)\n",
    "model.fit(df_train_2[df_var.index], df_train_2.iloc[:,-1:])\n",
    "\n",
    "# Prediction on training data\n",
    "y_train_pred = model.predict(df_train_2[df_var.index])\n",
    "train_rmse = np.round(RMSE(df_train_2.iloc[:,-1:], y_train_pred),3)\n",
    "\n",
    "# Prediction on test data\n",
    "y_test_pred = model.predict(df_test_2[df_var.index])\n",
    "test_rmse = np.round(RMSE(df_test_2.iloc[:,-1:], y_test_pred),3)\n",
    "\n",
    "print(f'Training RMSE for {drug2}: {train_rmse}')\n",
    "print(f'Test RMSE {drug2}: {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739554c3-b629-4fb3-aadd-25f2683ca9b6",
   "metadata": {},
   "source": [
    "4.3.3 For Entinostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a50d9a-4a8b-4afe-9b7e-d8d1fcd61b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n",
      "Best parameters found: {'colsample_bytree': 0.8, 'gamma': 0.5, 'learning_rate': 0.75, 'max_depth': 2, 'min_child_weight': 1, 'subsample': 0.8}\n",
      "Training RMSE for Entinostat: 0.093\n",
      "Test RMSE for Entinostat: 0.107\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train_3, df_test_3 = train_test_split(df_3, shuffle=True, random_state=42, test_size=.2) # For Entinostat\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_child_weight': [1, 5, 10], # minimum number of instances needed to be in each node\n",
    "    'gamma': [0.5, 1, 1.5], # minimum loss reduction required to make a further partition\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'learning_rate': [0.1, 0.25, 0.5, 0.75]\n",
    "}\n",
    "\n",
    "# cv : number of k-fold cross validation\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective ='reg:squarederror'),\n",
    "                           param_grid=params,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=3,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(df_train_3[df_var.index], df_train_3.iloc[:,-1:])\n",
    "print(f'Best parameters found: {grid_search.best_params_}')\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', **best_params)\n",
    "model.fit(df_train_3[df_var.index], df_train_3.iloc[:,-1:])\n",
    "\n",
    "# Prediction on training data\n",
    "y_train_pred = model.predict(df_train_3[df_var.index])\n",
    "train_rmse = np.round(RMSE(df_train_3.iloc[:,-1:], y_train_pred),3)\n",
    "\n",
    "# Prediction on test data\n",
    "y_test_pred = model.predict(df_test_3[df_var.index])\n",
    "test_rmse = np.round(RMSE(df_test_3.iloc[:,-1:], y_test_pred),3)\n",
    "\n",
    "print(f'Training RMSE for {drug3}: {train_rmse}')\n",
    "print(f'Test RMSE for {drug3}: {test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a70c5-1132-4373-9867-d30a0fedcaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
